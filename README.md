# Credit-Card-Transaction-Fraud-Analysis
## SUMMARY
The project focuses on Transaction fraud which happens during the account usage such as Credit card transactions, Money transfers from a compromised account (account takeover), Insurance claims (auto, P&C, healthcare…), Tax returns. The project aims to help understand the better model to choose for better savings from the happening of fraud. The report starts with a brief introduction of the dataset, the statistics, and field summary tables. The process of analysis starts with Data cleaning and the cleaning of each field is described in the report. It follows a series of steps such as variable generation, feature selection, model exploration, performance, and financial plots to understand the maximum savings. Each of these are elaborated on below, and as a threshold, a Fraud Detection Rate (FDR) of 3% is used. The report also provides reasoning for choosing the below model for the analysis. The brief conclusions and summary are provided in the report, with an appendix that includes the data quality report and the corresponding visualizations. The names mentioned in double quotations (“..”) indicate the field names.
## DATA DESCRIPTION
Dataset Name: Card Transactions.csv
The data is split into training, testing, and validation (out of time (oot – considering the last 2 months of data not involved in training or testing))
## METHODOLOGY
The series of steps followed in this project are as follows:
1. **Data Collection** : The transactions data has been obtained from the author Mark Nigrini's website. It is a collection of 100,000 data points and 10 fields from a supposed company that is based out of Tennesse. The data is considered to be real, small and messy. The 10 fields are - Recordnum, Cardnum, Date, Merchantnum, Merchant Description, Merchant state, Merchzip, Transtype, Amount, Fraud (binary field)
2. **Data Exploration and Cleaning** :  A Data Quality Report(DQR) is generated first to understand the time behaviour of transaction data, monthly volume and seasonality. The DQR is attached in the appendix of the final report. The data cleaning is done by observing the field summary tables and changing the necessary field types. The data points that are above a suitable choosen threshold are dropped. Null values are identified and are filled using field imputation by creating dictionaries of non null values.
3. **Feature Engineering** : Variables such as Day Since, Degree Centrality Network, Frequency & Amount, Velocity, Velocity Change, Velocity day since ratio, max indicator, acceleration, variability variables were generated to improve the perfomance of the machine learning model.
4. **Feature Selection** : Multiple filter and wrapper combinations were tested that included a combination of stepwise selection (forward/backward) with Light Gradient Boost Machine(LGBM) or Random Forest(RF), out of which the combination that gives the highest wrapper performance without overfitting is selected.
5. **Model Exploration** : Various models are tested by hyperparameter tuning, and they are compared on their performance in a performance plot. The model with highest perfomance is selected.
6. **Results Interpretation** : A financial plot is generated using the above selected model at 3% cutoff of Fraud Detection Rate to estimate the maximum savings incase of a fraud. 
